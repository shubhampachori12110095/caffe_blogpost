{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Tensorflow and keras. \n",
    "I am doing this just to download the MNIST dataset available on keras. If you know any other way. feel free to do it that way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-1.2.1-cp27-cp27mu-manylinux1_x86_64.whl (34.5MB)\n",
      "\u001b[K    100% |################################| 34.5MB 34kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting mock>=2.0.0 (from tensorflow)\n",
      "  Downloading mock-2.0.0-py2.py3-none-any.whl (56kB)\n",
      "\u001b[K    100% |################################| 61kB 315kB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.11.0 in /usr/lib/python2.7/dist-packages (from tensorflow)\n",
      "Collecting bleach==1.5.0 (from tensorflow)\n",
      "  Downloading bleach-1.5.0-py2.py3-none-any.whl\n",
      "Collecting markdown>=2.6.8 (from tensorflow)\n",
      "  Downloading Markdown-2.6.8.tar.gz (307kB)\n",
      "\u001b[K    100% |################################| 317kB 175kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting wheel (from tensorflow)\n",
      "  Downloading wheel-0.29.0-py2.py3-none-any.whl (66kB)\n",
      "\u001b[K    100% |################################| 71kB 242kB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow)\n",
      "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow)\n",
      "Collecting backports.weakref==1.0rc1 (from tensorflow)\n",
      "  Downloading backports.weakref-1.0rc1-py2-none-any.whl\n",
      "Collecting html5lib==0.9999999 (from tensorflow)\n",
      "  Downloading html5lib-0.9999999.tar.gz (889kB)\n",
      "\u001b[K    100% |################################| 890kB 443kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting werkzeug>=0.11.10 (from tensorflow)\n",
      "  Downloading Werkzeug-0.12.2-py2.py3-none-any.whl (312kB)\n",
      "\u001b[K    100% |################################| 317kB 498kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting funcsigs>=1; python_version < \"3.3\" (from mock>=2.0.0->tensorflow)\n",
      "  Downloading funcsigs-1.0.2-py2.py3-none-any.whl\n",
      "Collecting pbr>=0.11 (from mock>=2.0.0->tensorflow)\n",
      "  Downloading pbr-3.1.1-py2.py3-none-any.whl (99kB)\n",
      "\u001b[K    100% |################################| 102kB 353kB/s a 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/lib/python2.7/dist-packages (from protobuf>=3.2.0->tensorflow)\n",
      "Installing collected packages: funcsigs, pbr, mock, html5lib, bleach, markdown, wheel, backports.weakref, werkzeug, tensorflow\n",
      "  Found existing installation: html5lib 0.999999999\n",
      "    Uninstalling html5lib-0.999999999:\n",
      "      Successfully uninstalled html5lib-0.999999999\n",
      "  Running setup.py install for html5lib ... \u001b[?25ldone\n",
      "\u001b[?25h  Found existing installation: bleach 2.0.0\n",
      "    Uninstalling bleach-2.0.0:\n",
      "      Successfully uninstalled bleach-2.0.0\n",
      "  Running setup.py install for markdown ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed backports.weakref-1.0rc1 bleach-1.5.0 funcsigs-1.0.2 html5lib-0.9999999 markdown-2.6.8 mock-2.0.0 pbr-3.1.1 tensorflow-1.2.1 werkzeug-0.12.2 wheel-0.29.0\n",
      "Collecting keras\n",
      "  Downloading Keras-2.0.6.tar.gz (228kB)\n",
      "\u001b[K    100% |################################| 235kB 470kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting theano (from keras)\n",
      "  Downloading Theano-0.9.0.tar.gz (3.1MB)\n",
      "\u001b[K    100% |################################| 3.1MB 118kB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python2.7/dist-packages (from keras)\n",
      "Requirement already satisfied: six in /usr/local/lib/python2.7/dist-packages (from keras)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/lib/python2.7/dist-packages (from theano->keras)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/lib/python2.7/dist-packages (from theano->keras)\n",
      "Building wheels for collected packages: keras, theano\n",
      "  Running setup.py bdist_wheel for keras ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/c2/80/ba/2beab8c2131e2dcc391ee8a2f55e648af66348115c245e0839\n",
      "  Running setup.py bdist_wheel for theano ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/d5/5b/93/433299b86e3e9b25f0f600e4e4ebf18e38eb7534ea518eba13\n",
      "Successfully built keras theano\n",
      "Installing collected packages: theano, keras\n",
      "Successfully installed keras-2.0.6 theano-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow \n",
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Downloading data \n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Saving data in h5py format. \n",
    "with h5py.File('../data/train.h5', 'w') as f:\n",
    "    f['data'] = x_train / 255.0\n",
    "    f['label'] = y_train.reshape(-1, 1, 1, 1)\n",
    "\n",
    "with h5py.File('../data/test.h5', 'w') as f:\n",
    "   f['data'] = x_test / 255.0\n",
    "   f['label'] = y_test.reshape(-1, 1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train_data', (60000, 1, 28, 28), (60000, 1, 1, 1))\n",
      "('test_data', (10000, 28, 28, 1), (10000, 1, 1, 1))\n"
     ]
    }
   ],
   "source": [
    "f = h5py.File(\"../data/train.h5\", \"r\")\n",
    "print(\"train_data\", f[\"data\"][:].shape, f[\"label\"][:].shape)\n",
    "f.close()\n",
    "\n",
    "\n",
    "f = h5py.File(\"../data/test.h5\", \"r\")\n",
    "print(\"test_data\", f[\"data\"][:].shape, f[\"label\"][:].shape)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the caffe model using commandline . \n",
    "check \n",
    "- train_val.prototxt for model architecture code \n",
    "- train_solver.prototxt for model params \n",
    "\n",
    "Since I am using CPU, this will take time to run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0802 05:45:42.283427    86 caffe.cpp:211] Use CPU.\n",
      "I0802 05:45:42.283920    86 solver.cpp:44] Initializing solver from parameters: \n",
      "test_iter: 100\n",
      "test_interval: 500\n",
      "base_lr: 0.001\n",
      "display: 500\n",
      "max_iter: 2000\n",
      "lr_policy: \"fixed\"\n",
      "momentum: 0.9\n",
      "snapshot: 500\n",
      "snapshot_prefix: \"./snapshot/lenet_mnist\"\n",
      "solver_mode: CPU\n",
      "net: \"./train_val.prototxt\"\n",
      "train_state {\n",
      "  level: 0\n",
      "  stage: \"\"\n",
      "}\n",
      "momentum2: 0.999\n",
      "type: \"Adam\"\n",
      "I0802 05:45:42.284037    86 solver.cpp:87] Creating training net from net file: ./train_val.prototxt\n",
      "I0802 05:45:42.284229    86 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist\n",
      "I0802 05:45:42.284241    86 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy\n",
      "I0802 05:45:42.284312    86 net.cpp:51] Initializing net from parameters: \n",
      "name: \"LeNet\"\n",
      "state {\n",
      "  phase: TRAIN\n",
      "  level: 0\n",
      "  stage: \"\"\n",
      "}\n",
      "layer {\n",
      "  name: \"mnist\"\n",
      "  type: \"HDF5Data\"\n",
      "  top: \"data\"\n",
      "  top: \"label\"\n",
      "  include {\n",
      "    phase: TRAIN\n",
      "  }\n",
      "  hdf5_data_param {\n",
      "    source: \"./train_file_location.txt\"\n",
      "    batch_size: 100\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv1\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"data\"\n",
      "  top: \"conv1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 20\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool1\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"pool1\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 2\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv2\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"conv2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 50\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool2\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"pool2\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 2\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"ip1\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"pool2\"\n",
      "  top: \"ip1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 500\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu1\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"ip1\"\n",
      "  top: \"ip1\"\n",
      "}\n",
      "layer {\n",
      "  name: \"ip2\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"ip1\"\n",
      "  top: \"ip2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 10\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"loss\"\n",
      "  type: \"SoftmaxWithLoss\"\n",
      "  bottom: \"ip2\"\n",
      "  bottom: \"label\"\n",
      "  top: \"loss\"\n",
      "}\n",
      "I0802 05:45:42.284436    86 layer_factory.hpp:77] Creating layer mnist\n",
      "I0802 05:45:42.284476    86 net.cpp:84] Creating Layer mnist\n",
      "I0802 05:45:42.284484    86 net.cpp:380] mnist -> data\n",
      "I0802 05:45:42.284512    86 net.cpp:380] mnist -> label\n",
      "I0802 05:45:42.284528    86 hdf5_data_layer.cpp:80] Loading list of HDF5 filenames from: ./train_file_location.txt\n",
      "I0802 05:45:42.284564    86 hdf5_data_layer.cpp:94] Number of HDF5 files: 1\n",
      "I0802 05:45:42.297123    86 hdf5.cpp:32] Datatype class: H5T_FLOAT\n",
      "I0802 05:45:42.538784    86 hdf5.cpp:35] Datatype class: H5T_INTEGER\n",
      "I0802 05:45:42.539894    86 net.cpp:122] Setting up mnist\n",
      "I0802 05:45:42.539925    86 net.cpp:129] Top shape: 100 1 28 28 (78400)\n",
      "I0802 05:45:42.539933    86 net.cpp:129] Top shape: 100 1 1 1 (100)\n",
      "I0802 05:45:42.539938    86 net.cpp:137] Memory required for data: 314000\n",
      "I0802 05:45:42.539950    86 layer_factory.hpp:77] Creating layer conv1\n",
      "I0802 05:45:42.539990    86 net.cpp:84] Creating Layer conv1\n",
      "I0802 05:45:42.539999    86 net.cpp:406] conv1 <- data\n",
      "I0802 05:45:42.540024    86 net.cpp:380] conv1 -> conv1\n",
      "I0802 05:45:42.542448    86 net.cpp:122] Setting up conv1\n",
      "I0802 05:45:42.542481    86 net.cpp:129] Top shape: 100 20 24 24 (1152000)\n",
      "I0802 05:45:42.542486    86 net.cpp:137] Memory required for data: 4922000\n",
      "I0802 05:45:42.542513    86 layer_factory.hpp:77] Creating layer pool1\n",
      "I0802 05:45:42.542522    86 net.cpp:84] Creating Layer pool1\n",
      "I0802 05:45:42.542567    86 net.cpp:406] pool1 <- conv1\n",
      "I0802 05:45:42.542575    86 net.cpp:380] pool1 -> pool1\n",
      "I0802 05:45:42.542600    86 net.cpp:122] Setting up pool1\n",
      "I0802 05:45:42.542605    86 net.cpp:129] Top shape: 100 20 12 12 (288000)\n",
      "I0802 05:45:42.542608    86 net.cpp:137] Memory required for data: 6074000\n",
      "I0802 05:45:42.542611    86 layer_factory.hpp:77] Creating layer conv2\n",
      "I0802 05:45:42.542621    86 net.cpp:84] Creating Layer conv2\n",
      "I0802 05:45:42.542625    86 net.cpp:406] conv2 <- pool1\n",
      "I0802 05:45:42.542631    86 net.cpp:380] conv2 -> conv2\n",
      "I0802 05:45:42.542850    86 net.cpp:122] Setting up conv2\n",
      "I0802 05:45:42.542857    86 net.cpp:129] Top shape: 100 50 8 8 (320000)\n",
      "I0802 05:45:42.542861    86 net.cpp:137] Memory required for data: 7354000\n",
      "I0802 05:45:42.542868    86 layer_factory.hpp:77] Creating layer pool2\n",
      "I0802 05:45:42.542877    86 net.cpp:84] Creating Layer pool2\n",
      "I0802 05:45:42.542881    86 net.cpp:406] pool2 <- conv2\n",
      "I0802 05:45:42.542886    86 net.cpp:380] pool2 -> pool2\n",
      "I0802 05:45:42.542892    86 net.cpp:122] Setting up pool2\n",
      "I0802 05:45:42.542896    86 net.cpp:129] Top shape: 100 50 4 4 (80000)\n",
      "I0802 05:45:42.542901    86 net.cpp:137] Memory required for data: 7674000\n",
      "I0802 05:45:42.542903    86 layer_factory.hpp:77] Creating layer ip1\n",
      "I0802 05:45:42.542913    86 net.cpp:84] Creating Layer ip1\n",
      "I0802 05:45:42.542917    86 net.cpp:406] ip1 <- pool2\n",
      "I0802 05:45:42.542923    86 net.cpp:380] ip1 -> ip1\n",
      "I0802 05:45:42.546192    86 net.cpp:122] Setting up ip1\n",
      "I0802 05:45:42.546214    86 net.cpp:129] Top shape: 100 500 (50000)\n",
      "I0802 05:45:42.546221    86 net.cpp:137] Memory required for data: 7874000\n",
      "I0802 05:45:42.546242    86 layer_factory.hpp:77] Creating layer relu1\n",
      "I0802 05:45:42.546255    86 net.cpp:84] Creating Layer relu1\n",
      "I0802 05:45:42.546262    86 net.cpp:406] relu1 <- ip1\n",
      "I0802 05:45:42.546268    86 net.cpp:367] relu1 -> ip1 (in-place)\n",
      "I0802 05:45:42.546281    86 net.cpp:122] Setting up relu1\n",
      "I0802 05:45:42.546288    86 net.cpp:129] Top shape: 100 500 (50000)\n",
      "I0802 05:45:42.546291    86 net.cpp:137] Memory required for data: 8074000\n",
      "I0802 05:45:42.546295    86 layer_factory.hpp:77] Creating layer ip2\n",
      "I0802 05:45:42.546308    86 net.cpp:84] Creating Layer ip2\n",
      "I0802 05:45:42.546314    86 net.cpp:406] ip2 <- ip1\n",
      "I0802 05:45:42.546319    86 net.cpp:380] ip2 -> ip2\n",
      "I0802 05:45:42.546376    86 net.cpp:122] Setting up ip2\n",
      "I0802 05:45:42.546383    86 net.cpp:129] Top shape: 100 10 (1000)\n",
      "I0802 05:45:42.546388    86 net.cpp:137] Memory required for data: 8078000\n",
      "I0802 05:45:42.546394    86 layer_factory.hpp:77] Creating layer loss\n",
      "I0802 05:45:42.546402    86 net.cpp:84] Creating Layer loss\n",
      "I0802 05:45:42.546406    86 net.cpp:406] loss <- ip2\n",
      "I0802 05:45:42.546411    86 net.cpp:406] loss <- label\n",
      "I0802 05:45:42.546418    86 net.cpp:380] loss -> loss\n",
      "I0802 05:45:42.546444    86 layer_factory.hpp:77] Creating layer loss\n",
      "I0802 05:45:42.546470    86 net.cpp:122] Setting up loss\n",
      "I0802 05:45:42.546478    86 net.cpp:129] Top shape: (1)\n",
      "I0802 05:45:42.546483    86 net.cpp:132]     with loss weight 1\n",
      "I0802 05:45:42.546519    86 net.cpp:137] Memory required for data: 8078004\n",
      "I0802 05:45:42.546525    86 net.cpp:198] loss needs backward computation.\n",
      "I0802 05:45:42.546535    86 net.cpp:198] ip2 needs backward computation.\n",
      "I0802 05:45:42.546540    86 net.cpp:198] relu1 needs backward computation.\n",
      "I0802 05:45:42.546545    86 net.cpp:198] ip1 needs backward computation.\n",
      "I0802 05:45:42.546550    86 net.cpp:198] pool2 needs backward computation.\n",
      "I0802 05:45:42.546555    86 net.cpp:198] conv2 needs backward computation.\n",
      "I0802 05:45:42.546561    86 net.cpp:198] pool1 needs backward computation.\n",
      "I0802 05:45:42.546566    86 net.cpp:198] conv1 needs backward computation.\n",
      "I0802 05:45:42.546571    86 net.cpp:200] mnist does not need backward computation.\n",
      "I0802 05:45:42.546576    86 net.cpp:242] This network produces output loss\n",
      "I0802 05:45:42.546586    86 net.cpp:255] Network initialization done.\n",
      "I0802 05:45:42.546859    86 solver.cpp:172] Creating test net (#0) specified by net file: ./train_val.prototxt\n",
      "I0802 05:45:42.546890    86 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist\n",
      "I0802 05:45:42.547032    86 net.cpp:51] Initializing net from parameters: \n",
      "name: \"LeNet\"\n",
      "state {\n",
      "  phase: TEST\n",
      "}\n",
      "layer {\n",
      "  name: \"mnist\"\n",
      "  type: \"HDF5Data\"\n",
      "  top: \"data\"\n",
      "  top: \"label\"\n",
      "  include {\n",
      "    phase: TEST\n",
      "  }\n",
      "  hdf5_data_param {\n",
      "    source: \"./test_file_location.txt\"\n",
      "    batch_size: 100\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv1\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"data\"\n",
      "  top: \"conv1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 20\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool1\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"pool1\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 2\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv2\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"conv2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 50\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool2\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"pool2\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 2\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"ip1\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"pool2\"\n",
      "  top: \"ip1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 500\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu1\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"ip1\"\n",
      "  top: \"ip1\"\n",
      "}\n",
      "layer {\n",
      "  name: \"ip2\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"ip1\"\n",
      "  top: \"ip2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 10\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"accuracy\"\n",
      "  type: \"Accuracy\"\n",
      "  bottom: \"ip2\"\n",
      "  bottom: \"label\"\n",
      "  top: \"accuracy\"\n",
      "  include {\n",
      "    phase: TEST\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"loss\"\n",
      "  type: \"SoftmaxWithLoss\"\n",
      "  bottom: \"ip2\"\n",
      "  bottom: \"label\"\n",
      "  top: \"loss\"\n",
      "}\n",
      "I0802 05:45:42.547183    86 layer_factory.hpp:77] Creating layer mnist\n",
      "I0802 05:45:42.547194    86 net.cpp:84] Creating Layer mnist\n",
      "I0802 05:45:42.547200    86 net.cpp:380] mnist -> data\n",
      "I0802 05:45:42.547209    86 net.cpp:380] mnist -> label\n",
      "I0802 05:45:42.547217    86 hdf5_data_layer.cpp:80] Loading list of HDF5 filenames from: ./test_file_location.txt\n",
      "I0802 05:45:42.547252    86 hdf5_data_layer.cpp:94] Number of HDF5 files: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0802 05:45:42.612730    86 net.cpp:122] Setting up mnist\n",
      "I0802 05:45:42.612782    86 net.cpp:129] Top shape: 100 1 28 28 (78400)\n",
      "I0802 05:45:42.612790    86 net.cpp:129] Top shape: 100 1 1 1 (100)\n",
      "I0802 05:45:42.612794    86 net.cpp:137] Memory required for data: 314000\n",
      "I0802 05:45:42.612802    86 layer_factory.hpp:77] Creating layer label_mnist_1_split\n",
      "I0802 05:45:42.612818    86 net.cpp:84] Creating Layer label_mnist_1_split\n",
      "I0802 05:45:42.612823    86 net.cpp:406] label_mnist_1_split <- label\n",
      "I0802 05:45:42.612833    86 net.cpp:380] label_mnist_1_split -> label_mnist_1_split_0\n",
      "I0802 05:45:42.612846    86 net.cpp:380] label_mnist_1_split -> label_mnist_1_split_1\n",
      "I0802 05:45:42.612855    86 net.cpp:122] Setting up label_mnist_1_split\n",
      "I0802 05:45:42.612860    86 net.cpp:129] Top shape: 100 1 1 1 (100)\n",
      "I0802 05:45:42.612864    86 net.cpp:129] Top shape: 100 1 1 1 (100)\n",
      "I0802 05:45:42.612870    86 net.cpp:137] Memory required for data: 314800\n",
      "I0802 05:45:42.612875    86 layer_factory.hpp:77] Creating layer conv1\n",
      "I0802 05:45:42.612891    86 net.cpp:84] Creating Layer conv1\n",
      "I0802 05:45:42.612896    86 net.cpp:406] conv1 <- data\n",
      "I0802 05:45:42.612905    86 net.cpp:380] conv1 -> conv1\n",
      "I0802 05:45:42.612947    86 net.cpp:122] Setting up conv1\n",
      "I0802 05:45:42.612956    86 net.cpp:129] Top shape: 100 20 24 24 (1152000)\n",
      "I0802 05:45:42.612959    86 net.cpp:137] Memory required for data: 4922800\n",
      "I0802 05:45:42.612970    86 layer_factory.hpp:77] Creating layer pool1\n",
      "I0802 05:45:42.613029    86 net.cpp:84] Creating Layer pool1\n",
      "I0802 05:45:42.613034    86 net.cpp:406] pool1 <- conv1\n",
      "I0802 05:45:42.613039    86 net.cpp:380] pool1 -> pool1\n",
      "I0802 05:45:42.613049    86 net.cpp:122] Setting up pool1\n",
      "I0802 05:45:42.613055    86 net.cpp:129] Top shape: 100 20 12 12 (288000)\n",
      "I0802 05:45:42.613059    86 net.cpp:137] Memory required for data: 6074800\n",
      "I0802 05:45:42.613061    86 layer_factory.hpp:77] Creating layer conv2\n",
      "I0802 05:45:42.613070    86 net.cpp:84] Creating Layer conv2\n",
      "I0802 05:45:42.613075    86 net.cpp:406] conv2 <- pool1\n",
      "I0802 05:45:42.613080    86 net.cpp:380] conv2 -> conv2\n",
      "I0802 05:45:42.613268    86 net.cpp:122] Setting up conv2\n",
      "I0802 05:45:42.613276    86 net.cpp:129] Top shape: 100 50 8 8 (320000)\n",
      "I0802 05:45:42.613281    86 net.cpp:137] Memory required for data: 7354800\n",
      "I0802 05:45:42.613289    86 layer_factory.hpp:77] Creating layer pool2\n",
      "I0802 05:45:42.613296    86 net.cpp:84] Creating Layer pool2\n",
      "I0802 05:45:42.613301    86 net.cpp:406] pool2 <- conv2\n",
      "I0802 05:45:42.613304    86 net.cpp:380] pool2 -> pool2\n",
      "I0802 05:45:42.613310    86 net.cpp:122] Setting up pool2\n",
      "I0802 05:45:42.613315    86 net.cpp:129] Top shape: 100 50 4 4 (80000)\n",
      "I0802 05:45:42.613318    86 net.cpp:137] Memory required for data: 7674800\n",
      "I0802 05:45:42.613322    86 layer_factory.hpp:77] Creating layer ip1\n",
      "I0802 05:45:42.613330    86 net.cpp:84] Creating Layer ip1\n",
      "I0802 05:45:42.613334    86 net.cpp:406] ip1 <- pool2\n",
      "I0802 05:45:42.613340    86 net.cpp:380] ip1 -> ip1\n",
      "I0802 05:45:42.616454    86 net.cpp:122] Setting up ip1\n",
      "I0802 05:45:42.616479    86 net.cpp:129] Top shape: 100 500 (50000)\n",
      "I0802 05:45:42.616485    86 net.cpp:137] Memory required for data: 7874800\n",
      "I0802 05:45:42.616499    86 layer_factory.hpp:77] Creating layer relu1\n",
      "I0802 05:45:42.616508    86 net.cpp:84] Creating Layer relu1\n",
      "I0802 05:45:42.616513    86 net.cpp:406] relu1 <- ip1\n",
      "I0802 05:45:42.616518    86 net.cpp:367] relu1 -> ip1 (in-place)\n",
      "I0802 05:45:42.616526    86 net.cpp:122] Setting up relu1\n",
      "I0802 05:45:42.616531    86 net.cpp:129] Top shape: 100 500 (50000)\n",
      "I0802 05:45:42.616534    86 net.cpp:137] Memory required for data: 8074800\n",
      "I0802 05:45:42.616538    86 layer_factory.hpp:77] Creating layer ip2\n",
      "I0802 05:45:42.616546    86 net.cpp:84] Creating Layer ip2\n",
      "I0802 05:45:42.616550    86 net.cpp:406] ip2 <- ip1\n",
      "I0802 05:45:42.616555    86 net.cpp:380] ip2 -> ip2\n",
      "I0802 05:45:42.616605    86 net.cpp:122] Setting up ip2\n",
      "I0802 05:45:42.616613    86 net.cpp:129] Top shape: 100 10 (1000)\n",
      "I0802 05:45:42.616617    86 net.cpp:137] Memory required for data: 8078800\n",
      "I0802 05:45:42.616624    86 layer_factory.hpp:77] Creating layer ip2_ip2_0_split\n",
      "I0802 05:45:42.616631    86 net.cpp:84] Creating Layer ip2_ip2_0_split\n",
      "I0802 05:45:42.616636    86 net.cpp:406] ip2_ip2_0_split <- ip2\n",
      "I0802 05:45:42.616642    86 net.cpp:380] ip2_ip2_0_split -> ip2_ip2_0_split_0\n",
      "I0802 05:45:42.616649    86 net.cpp:380] ip2_ip2_0_split -> ip2_ip2_0_split_1\n",
      "I0802 05:45:42.616657    86 net.cpp:122] Setting up ip2_ip2_0_split\n",
      "I0802 05:45:42.616662    86 net.cpp:129] Top shape: 100 10 (1000)\n",
      "I0802 05:45:42.616683    86 net.cpp:129] Top shape: 100 10 (1000)\n",
      "I0802 05:45:42.616688    86 net.cpp:137] Memory required for data: 8086800\n",
      "I0802 05:45:42.616693    86 layer_factory.hpp:77] Creating layer accuracy\n",
      "I0802 05:45:42.616703    86 net.cpp:84] Creating Layer accuracy\n",
      "I0802 05:45:42.616708    86 net.cpp:406] accuracy <- ip2_ip2_0_split_0\n",
      "I0802 05:45:42.616714    86 net.cpp:406] accuracy <- label_mnist_1_split_0\n",
      "I0802 05:45:42.616719    86 net.cpp:380] accuracy -> accuracy\n",
      "I0802 05:45:42.616727    86 net.cpp:122] Setting up accuracy\n",
      "I0802 05:45:42.616734    86 net.cpp:129] Top shape: (1)\n",
      "I0802 05:45:42.616737    86 net.cpp:137] Memory required for data: 8086804\n",
      "I0802 05:45:42.616742    86 layer_factory.hpp:77] Creating layer loss\n",
      "I0802 05:45:42.616747    86 net.cpp:84] Creating Layer loss\n",
      "I0802 05:45:42.616751    86 net.cpp:406] loss <- ip2_ip2_0_split_1\n",
      "I0802 05:45:42.616755    86 net.cpp:406] loss <- label_mnist_1_split_1\n",
      "I0802 05:45:42.616760    86 net.cpp:380] loss -> loss\n",
      "I0802 05:45:42.616807    86 layer_factory.hpp:77] Creating layer loss\n",
      "I0802 05:45:42.616824    86 net.cpp:122] Setting up loss\n",
      "I0802 05:45:42.616830    86 net.cpp:129] Top shape: (1)\n",
      "I0802 05:45:42.616834    86 net.cpp:132]     with loss weight 1\n",
      "I0802 05:45:42.616855    86 net.cpp:137] Memory required for data: 8086808\n",
      "I0802 05:45:42.616860    86 net.cpp:198] loss needs backward computation.\n",
      "I0802 05:45:42.616866    86 net.cpp:200] accuracy does not need backward computation.\n",
      "I0802 05:45:42.616873    86 net.cpp:198] ip2_ip2_0_split needs backward computation.\n",
      "I0802 05:45:42.616878    86 net.cpp:198] ip2 needs backward computation.\n",
      "I0802 05:45:42.616883    86 net.cpp:198] relu1 needs backward computation.\n",
      "I0802 05:45:42.616888    86 net.cpp:198] ip1 needs backward computation.\n",
      "I0802 05:45:42.616892    86 net.cpp:198] pool2 needs backward computation.\n",
      "I0802 05:45:42.616896    86 net.cpp:198] conv2 needs backward computation.\n",
      "I0802 05:45:42.616901    86 net.cpp:198] pool1 needs backward computation.\n",
      "I0802 05:45:42.616906    86 net.cpp:198] conv1 needs backward computation.\n",
      "I0802 05:45:42.616911    86 net.cpp:200] label_mnist_1_split does not need backward computation.\n",
      "I0802 05:45:42.616917    86 net.cpp:200] mnist does not need backward computation.\n",
      "I0802 05:45:42.616921    86 net.cpp:242] This network produces output accuracy\n",
      "I0802 05:45:42.616927    86 net.cpp:242] This network produces output loss\n",
      "I0802 05:45:42.616938    86 net.cpp:255] Network initialization done.\n",
      "I0802 05:45:42.617233    86 solver.cpp:56] Solver scaffolding done.\n",
      "I0802 05:45:42.617274    86 caffe.cpp:248] Starting Optimization\n",
      "I0802 05:45:42.617280    86 solver.cpp:272] Solving LeNet\n",
      "I0802 05:45:42.617285    86 solver.cpp:273] Learning Rate Policy: fixed\n",
      "I0802 05:45:42.617898    86 solver.cpp:330] Iteration 0, Testing net (#0)\n",
      "I0802 05:45:53.415305    86 solver.cpp:397]     Test net output #0: accuracy = 0.0983\n",
      "I0802 05:45:53.415375    86 solver.cpp:397]     Test net output #1: loss = 2.31185 (* 1 = 2.31185 loss)\n",
      "I0802 05:45:53.622025    86 solver.cpp:218] Iteration 0 (-6.55516e+21 iter/s, 11.004s/500 iters), loss = 2.34027\n",
      "I0802 05:45:53.622062    86 solver.cpp:237]     Train net output #0: loss = 2.34027 (* 1 = 2.34027 loss)\n",
      "I0802 05:45:53.622071    86 sgd_solver.cpp:105] Iteration 0, lr = 0.001\n",
      "I0802 05:47:21.758249    86 solver.cpp:447] Snapshotting to binary proto file ./snapshot/lenet_mnist_iter_500.caffemodel\n",
      "I0802 05:47:21.769733    86 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshot/lenet_mnist_iter_500.solverstate\n",
      "I0802 05:47:21.777535    86 solver.cpp:330] Iteration 500, Testing net (#0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0802 05:47:28.131386    86 solver.cpp:397]     Test net output #0: accuracy = 0.9809\n",
      "I0802 05:47:28.131458    86 solver.cpp:397]     Test net output #1: loss = 0.0617289 (* 1 = 0.0617289 loss)\n",
      "I0802 05:47:28.291963    86 solver.cpp:218] Iteration 500 (5.28156 iter/s, 94.669s/500 iters), loss = 0.061748\n",
      "I0802 05:47:28.292059    86 solver.cpp:237]     Train net output #0: loss = 0.0617481 (* 1 = 0.0617481 loss)\n",
      "I0802 05:47:28.292073    86 sgd_solver.cpp:105] Iteration 500, lr = 0.001\n",
      "I0802 05:48:59.335649    86 solver.cpp:447] Snapshotting to binary proto file ./snapshot/lenet_mnist_iter_1000.caffemodel\n",
      "I0802 05:48:59.347698    86 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshot/lenet_mnist_iter_1000.solverstate\n",
      "I0802 05:48:59.353976    86 solver.cpp:330] Iteration 1000, Testing net (#0)\n",
      "I0802 05:49:05.602417    86 solver.cpp:397]     Test net output #0: accuracy = 0.9871\n",
      "I0802 05:49:05.602502    86 solver.cpp:397]     Test net output #1: loss = 0.0439339 (* 1 = 0.0439339 loss)\n",
      "I0802 05:49:05.760406    86 solver.cpp:218] Iteration 1000 (5.12989 iter/s, 97.468s/500 iters), loss = 0.0424808\n",
      "I0802 05:49:05.760473    86 solver.cpp:237]     Train net output #0: loss = 0.0424808 (* 1 = 0.0424808 loss)\n",
      "I0802 05:49:05.760483    86 sgd_solver.cpp:105] Iteration 1000, lr = 0.001\n",
      "I0802 05:50:36.440356    86 solver.cpp:447] Snapshotting to binary proto file ./snapshot/lenet_mnist_iter_1500.caffemodel\n",
      "I0802 05:50:36.454258    86 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshot/lenet_mnist_iter_1500.solverstate\n",
      "I0802 05:50:36.461207    86 solver.cpp:330] Iteration 1500, Testing net (#0)\n",
      "I0802 05:50:42.245474    86 solver.cpp:397]     Test net output #0: accuracy = 0.987\n",
      "I0802 05:50:42.245544    86 solver.cpp:397]     Test net output #1: loss = 0.0416985 (* 1 = 0.0416985 loss)\n",
      "I0802 05:50:42.408715    86 solver.cpp:218] Iteration 1500 (5.17341 iter/s, 96.648s/500 iters), loss = 0.0194174\n",
      "I0802 05:50:42.408790    86 solver.cpp:237]     Train net output #0: loss = 0.0194174 (* 1 = 0.0194174 loss)\n",
      "I0802 05:50:42.408802    86 sgd_solver.cpp:105] Iteration 1500, lr = 0.001\n",
      "I0802 05:52:14.117521    86 solver.cpp:447] Snapshotting to binary proto file ./snapshot/lenet_mnist_iter_2000.caffemodel\n",
      "I0802 05:52:14.128043    86 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshot/lenet_mnist_iter_2000.solverstate\n",
      "I0802 05:52:14.201176    86 solver.cpp:310] Iteration 2000, loss = 0.0720342\n",
      "I0802 05:52:14.201230    86 solver.cpp:330] Iteration 2000, Testing net (#0)\n",
      "I0802 05:52:20.179733    86 solver.cpp:397]     Test net output #0: accuracy = 0.9878\n",
      "I0802 05:52:20.179760    86 solver.cpp:397]     Test net output #1: loss = 0.042841 (* 1 = 0.042841 loss)\n",
      "I0802 05:52:20.179765    86 solver.cpp:315] Optimization Done.\n",
      "I0802 05:52:20.179769    86 caffe.cpp:259] Optimization Done.\n"
     ]
    }
   ],
   "source": [
    "!caffe train --solver=./train_solver.prototxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important things to know.\n",
    "- when defining params in train_solver.prototxt, I have mentioned snapshot as 5000. this means there will be a weight file saved after every 5000 iterations. You need to check the logs of where the model has high validation accuracy and use it for deploying the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./snapshot/lenet_mnist_iter_500.solverstate',\n",
       " './snapshot/lenet_mnist_iter_1500.caffemodel',\n",
       " './snapshot/lenet_mnist_iter_2970.solverstate',\n",
       " './snapshot/lenet_mnist_iter_500.caffemodel',\n",
       " './snapshot/lenet_mnist_iter_2000.solverstate',\n",
       " './snapshot/lenet_mnist_iter_1000.solverstate',\n",
       " './snapshot/lenet_mnist_iter_1000.caffemodel',\n",
       " './snapshot/lenet_mnist_iter_2970.caffemodel',\n",
       " './snapshot/lenet_mnist_iter_2000.caffemodel',\n",
       " './snapshot/lenet_mnist_iter_1500.solverstate']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "glob.glob(\"./snapshot/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('test_data', (10000, 28, 28, 1), (10000, 1, 1, 1))\n",
      "[7 2 1 ..., 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "import caffe \n",
    "\n",
    "# Define the location of model and weights \n",
    "model = \"./deploy.prototxt\"\n",
    "weights = \"./snapshot/lenet_mnist_iter_2970.caffemodel\"\n",
    "\n",
    "# start the classifier\n",
    "net = caffe.Classifier(model, weights)\n",
    "\n",
    "## load the test file \n",
    "f = h5py.File(\"../data/test.h5\", \"r\")\n",
    "print(\"test_data\", f[\"data\"][:].shape, f[\"label\"][:].shape)\n",
    "\n",
    "# Seperate the input and label data. \n",
    "data = f[\"data\"][:]\n",
    "actual_label = f[\"label\"][:]\n",
    "\n",
    "# reshape and print the actual label \n",
    "y_test = actual_label.reshape(-1)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "pred = net.predict(data)\n",
    "print(pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 2 1 ..., 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "y_pred = np.argmax(pred, axis =1)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9602\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy: \" + str(accuracy_score(y_pred=y_pred, y_true=y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 971    0    0    0    0    1    3    2    2    1]\n",
      " [   0 1124    5    0    1    1    0    0    4    0]\n",
      " [   4    0  971    0    0   47    5    3    2    0]\n",
      " [   1    0    8  953    0   15    3    1   27    2]\n",
      " [   1    0    3    0  967    1    4    0    1    5]\n",
      " [   4    0   33    4    0  845    2    2    2    0]\n",
      " [   5    2   44    3    1    2  897    0    4    0]\n",
      " [   7    1    0    0    0   61    3  951    1    4]\n",
      " [   0    0    2    0    1    5    0    0  963    3]\n",
      " [   5    0    3    0    4    2    0    2   33  960]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
